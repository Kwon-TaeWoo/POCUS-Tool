#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
C# WPF ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import argparse
import json
from typing import Tuple, Dict, Any

# í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ python_modulesì—ì„œ import
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, 'python_modules'))

try:
    from python_modules.model import VisionTransformer, CONFIGS
    from python_modules.model_init import model_load_ViT
    from python_modules.gpu_optimizer import get_gpu_optimizer
except ImportError as e:
    print(f"Import error: {e}")
    print("Trying alternative import...")
    from model import VisionTransformer, CONFIGS
    from model_init import model_load_ViT
    from gpu_optimizer import get_gpu_optimizer


class PyTorchToONNXConverter:
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.gpu_optimizer = get_gpu_optimizer()
        self.device = self.gpu_optimizer.device
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        print(self.gpu_optimizer.get_optimization_report())
    
    def convert_model(self, 
                     model_path: str, 
                     output_path: str,
                     input_size: Tuple[int, int, int] = (256, 256, 3),
                     opset_version: int = 11) -> bool:
        """
        PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        
        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pth)
            output_path: ì¶œë ¥ ONNX íŒŒì¼ ê²½ë¡œ (.onnx)
            input_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (H, W, C)
            opset_version: ONNX opset ë²„ì „
            
        Returns:
            bool: ë³€í™˜ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path} -> {output_path}")
            
            # 1. PyTorch ëª¨ë¸ ë¡œë“œ
            model, transforms = self._load_pytorch_model(model_path, input_size)
            if model is None:
                return False
            
            # 2. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            model.eval()
            
            # 3. ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = torch.randn(1, 3, input_size[0], input_size[1]).to(self.device)
            print(f"ë”ë¯¸ ì…ë ¥ í¬ê¸°: {dummy_input.shape}")
            
            # 4. ONNXë¡œ ë³€í™˜
            print("ONNX ë³€í™˜ ì¤‘...")
            torch.onnx.export(
                model,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                },
                verbose=False
            )
            
            # 5. ë³€í™˜ëœ ëª¨ë¸ ê²€ì¦
            if self._validate_onnx_model(output_path, dummy_input):
                print(f"âœ… ONNX ë³€í™˜ ì„±ê³µ: {output_path}")
                
                # 6. ëª¨ë¸ ì •ë³´ ì €ì¥
                model_info = self._get_model_info(model, input_size, output_path)
                self._save_model_info(output_path.replace('.onnx', '_info.json'), model_info)
                
                return True
            else:
                print("âŒ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _load_pytorch_model(self, model_path: str, input_size: Tuple[int, int, int]):
        """PyTorch ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"PyTorch ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
            
            # ê¸°ì¡´ model_load_ViT í•¨ìˆ˜ ì‚¬ìš©
            model, transforms = model_load_ViT(model_path, input_size)
            
            if model is None:
                print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return None, None
            
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            model = model.to(self.device)
            
            print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
            
            return model, transforms
            
        except Exception as e:
            print(f"PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _validate_onnx_model(self, onnx_path: str, dummy_input: torch.Tensor) -> bool:
        """ë³€í™˜ëœ ONNX ëª¨ë¸ ê²€ì¦"""
        try:
            import onnxruntime as ort
            
            # ONNX Runtimeìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
            session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])
            
            # ì…ë ¥ ì´ë¦„ í™•ì¸
            input_name = session.get_inputs()[0].name
            output_name = session.get_outputs()[0].name
            
            print(f"ONNX ì…ë ¥ ì´ë¦„: {input_name}")
            print(f"ONNX ì¶œë ¥ ì´ë¦„: {output_name}")
            
            # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            input_data = dummy_input.cpu().numpy()
            outputs = session.run([output_name], {input_name: input_data})
            
            print(f"ONNX ì¶œë ¥ í¬ê¸°: {outputs[0].shape}")
            print("âœ… ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ")
            
            return True
            
        except Exception as e:
            print(f"ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _get_model_info(self, model: torch.nn.Module, input_size: Tuple[int, int, int], output_path: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            'model_type': 'VisionTransformer',
            'input_size': input_size,
            'output_path': output_path,
            'parameters_count': sum(p.numel() for p in model.parameters()),
            'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),
            'device': str(self.device),
            'opset_version': 11,
            'input_names': ['input'],
            'output_names': ['output'],
            'preprocessing': {
                'resize': input_size[:2],
                'normalize': {
                    'mean': [0.485, 0.456, 0.406],
                    'std': [0.229, 0.224, 0.225]
                }
            },
            'postprocessing': {
                'resize_output': [500, 460],
                'multiply_factor': 120
            }
        }
        
        return info
    
    def _save_model_info(self, info_path: str, model_info: Dict[str, Any]):
        """ëª¨ë¸ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            print(f"ëª¨ë¸ ì •ë³´ ì €ì¥: {info_path}")
        except Exception as e:
            print(f"ëª¨ë¸ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜')
    parser.add_argument('--model_path', type=str, required=True,
                       help='PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pth)')
    parser.add_argument('--output_path', type=str, required=True,
                       help='ì¶œë ¥ ONNX íŒŒì¼ ê²½ë¡œ (.onnx)')
    parser.add_argument('--input_size', type=int, nargs=3, default=[256, 256, 3],
                       help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (H W C)')
    parser.add_argument('--opset_version', type=int, default=11,
                       help='ONNX opset ë²„ì „')
    
    args = parser.parse_args()
    
    # ì…ë ¥ í¬ê¸° íŠœí”Œë¡œ ë³€í™˜
    input_size = tuple(args.input_size)
    
    # ë³€í™˜ê¸° ìƒì„± ë° ì‹¤í–‰
    converter = PyTorchToONNXConverter()
    
    success = converter.convert_model(
        model_path=args.model_path,
        output_path=args.output_path,
        input_size=input_size,
        opset_version=args.opset_version
    )
    
    if success:
        print("\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
        print(f"ONNX ëª¨ë¸: {args.output_path}")
        print(f"ëª¨ë¸ ì •ë³´: {args.output_path.replace('.onnx', '_info.json')}")
    else:
        print("\nâŒ ë³€í™˜ ì‹¤íŒ¨!")
        sys.exit(1)


if __name__ == "__main__":
    main()
